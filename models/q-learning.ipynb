{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "# 1. Data Loading and Preprocessing\n",
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"\n",
    "    Load stock data and preprocess it for Q-learning\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Ensure date is in datetime format\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # Sort by date\n",
    "    df = df.sort_values('date')\n",
    "    \n",
    "    # Calculate additional features\n",
    "    df['return'] = df['close'].pct_change()\n",
    "    df['return_5d'] = df['close'].pct_change(periods=5)\n",
    "    df['volatility'] = df['return'].rolling(window=5).std()\n",
    "    \n",
    "    # Drop rows with NaN values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 2. State Representation\n",
    "def get_state(data, t, n_days):\n",
    "    \"\"\"\n",
    "    Create a state representation at time t using the past n_days\n",
    "    State includes: price trends, volatility, and volume patterns\n",
    "    \"\"\"\n",
    "    # Check if we have enough data\n",
    "    if t >= len(data):\n",
    "        t = len(data) - 1\n",
    "    \n",
    "    if t < n_days:\n",
    "        # Pad with zeros if not enough history\n",
    "        return np.zeros(3)\n",
    "    \n",
    "    # Price trend: -1 (downtrend), 0 (sideways), 1 (uptrend)\n",
    "    if data['close'].iloc[t] > data['close'].iloc[t-n_days] * 1.05:\n",
    "        price_trend = 1  # uptrend\n",
    "    elif data['close'].iloc[t] < data['close'].iloc[t-n_days] * 0.95:\n",
    "        price_trend = -1  # downtrend\n",
    "    else:\n",
    "        price_trend = 0  # sideways\n",
    "    \n",
    "    # Volatility: 0 (low), 1 (medium), 2 (high)\n",
    "    recent_volatility = data['volatility'].iloc[t]\n",
    "    if recent_volatility < 0.01:\n",
    "        volatility_state = 0\n",
    "    elif recent_volatility < 0.03:\n",
    "        volatility_state = 1\n",
    "    else:\n",
    "        volatility_state = 2\n",
    "    \n",
    "    # Volume: 0 (low), 1 (normal), 2 (high)\n",
    "    avg_volume = data['volume'].iloc[t-n_days:t].mean()\n",
    "    current_volume = data['volume'].iloc[t]\n",
    "    if current_volume > avg_volume * 1.5:\n",
    "        volume_state = 2\n",
    "    elif current_volume < avg_volume * 0.5:\n",
    "        volume_state = 0\n",
    "    else:\n",
    "        volume_state = 1\n",
    "    \n",
    "    return np.array([price_trend, volatility_state, volume_state])\n",
    "\n",
    "# 3. Define Actions\n",
    "# 0: Hold, 1: Buy, 2: Sell\n",
    "num_actions = 3\n",
    "\n",
    "# 4. Q-Learning Agent\n",
    "class QLearningAgent:\n",
    "    def __init__(self, state_size, action_size, learning_rate=0.1, discount_factor=0.95, exploration_rate=1.0, exploration_decay=0.995):\n",
    "        # Initialize Q-table with zeros\n",
    "        # 3 price trends x 3 volatility states x 3 volume states x 3 actions\n",
    "        self.q_table = np.zeros((3, 3, 3, action_size))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.exploration_decay = exploration_decay\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        \"\"\"Select action using epsilon-greedy policy\"\"\"\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            # Explore: choose random action\n",
    "            return random.randrange(num_actions)\n",
    "        else:\n",
    "            # Exploit: choose best action from Q-table\n",
    "            return np.argmax(self.q_table[int(state[0]+1), int(state[1]), int(state[2]), :])\n",
    "    \n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        \"\"\"Update Q-value using the Bellman equation\"\"\"\n",
    "        # Clip reward to prevent explosion\n",
    "        reward = np.clip(reward, -100, 100)\n",
    "        \n",
    "        # Current Q-value\n",
    "        current_q = self.q_table[int(state[0]+1), int(state[1]), int(state[2]), action]\n",
    "        \n",
    "        # Maximum Q-value for next state\n",
    "        max_next_q = np.max(self.q_table[int(next_state[0]+1), int(next_state[1]), int(next_state[2]), :])\n",
    "        \n",
    "        # Bellman equation for Q-value update\n",
    "        new_q = current_q + self.learning_rate * (reward + self.discount_factor * max_next_q - current_q)\n",
    "        \n",
    "        # Update Q-table\n",
    "        self.q_table[int(state[0]+1), int(state[1]), int(state[2]), action] = new_q\n",
    "    \n",
    "    def decay_exploration(self):\n",
    "        \"\"\"Decay exploration rate\"\"\"\n",
    "        self.exploration_rate *= self.exploration_decay\n",
    "        self.exploration_rate = max(0.01, self.exploration_rate)  # Set minimum exploration rate\n",
    "\n",
    "# 5. Trading Environment\n",
    "class TradingEnvironment:\n",
    "    def __init__(self, data, initial_balance=10000, transaction_fee_percent=0.1):\n",
    "        self.data = data\n",
    "        self.initial_balance = initial_balance\n",
    "        self.transaction_fee_percent = transaction_fee_percent\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment for a new episode\"\"\"\n",
    "        self.balance = self.initial_balance\n",
    "        self.shares_held = 0\n",
    "        self.current_step = 0\n",
    "        self.net_worth_history = [self.initial_balance]\n",
    "        self.avg_buy_price = 0\n",
    "        return get_state(self.data, self.current_step, n_days=5)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Execute action and return new state, reward, and done flag\"\"\"\n",
    "        # Get current price\n",
    "        current_price = self.data['close'].iloc[self.current_step]\n",
    "        \n",
    "        # Check for NaN price\n",
    "        if pd.isna(current_price):\n",
    "            # Skip this step if price is NaN\n",
    "            self.current_step += 1\n",
    "            return get_state(self.data, self.current_step, n_days=5), 0, self.current_step >= len(self.data) - 1\n",
    "        \n",
    "        # Initialize reward\n",
    "        reward = 0\n",
    "        \n",
    "        # Execute action\n",
    "        if action == 1:  # Buy\n",
    "            # Calculate maximum shares that can be bought\n",
    "            max_shares = self.balance / current_price\n",
    "            # Ensure max_shares is not NaN\n",
    "            if not pd.isna(max_shares):\n",
    "                shares_to_buy = int(max_shares)  # Convert to integer safely\n",
    "                \n",
    "                if shares_to_buy > 0:\n",
    "                    # Calculate transaction fee\n",
    "                    transaction_fee = shares_to_buy * current_price * (self.transaction_fee_percent / 100)\n",
    "                    \n",
    "                    # Update balance and shares held\n",
    "                    self.balance -= (shares_to_buy * current_price + transaction_fee)\n",
    "                    self.shares_held += shares_to_buy\n",
    "                    \n",
    "                    # Update average buy price\n",
    "                    self.avg_buy_price = current_price\n",
    "                    \n",
    "                    # Penalize for transaction fee\n",
    "                    reward -= transaction_fee / self.initial_balance  # Normalize reward\n",
    "                    \n",
    "        elif action == 2:  # Sell\n",
    "            if self.shares_held > 0:\n",
    "                # Calculate transaction fee\n",
    "                transaction_fee = self.shares_held * current_price * (self.transaction_fee_percent / 100)\n",
    "                \n",
    "                # Calculate profit/loss\n",
    "                profit_loss = (current_price - self.avg_buy_price) * self.shares_held\n",
    "                \n",
    "                # Update balance and shares held\n",
    "                self.balance += (self.shares_held * current_price - transaction_fee)\n",
    "                \n",
    "                # Add profit/loss to reward (normalized)\n",
    "                reward += profit_loss / self.initial_balance  # Normalize reward\n",
    "                \n",
    "                # Penalize for transaction fee\n",
    "                reward -= transaction_fee / self.initial_balance  # Normalize reward\n",
    "                \n",
    "                # Reset shares held\n",
    "                self.shares_held = 0\n",
    "        \n",
    "        # Move to next step\n",
    "        self.current_step += 1\n",
    "        \n",
    "        # Calculate net worth\n",
    "        net_worth = self.balance + self.shares_held * current_price\n",
    "        self.net_worth_history.append(net_worth)\n",
    "        \n",
    "        # Calculate additional reward based on portfolio growth\n",
    "        prev_net_worth = self.net_worth_history[-2]\n",
    "        if prev_net_worth > 0:  # Prevent division by zero\n",
    "            pct_change = (net_worth - prev_net_worth) / prev_net_worth\n",
    "            reward += pct_change  # No additional scaling needed\n",
    "        \n",
    "        # Check if done\n",
    "        done = self.current_step >= len(self.data) - 1\n",
    "        \n",
    "        # Get next state\n",
    "        next_state = get_state(self.data, self.current_step, n_days=5)\n",
    "        \n",
    "        return next_state, reward, done\n",
    "\n",
    "# 6. Train the model\n",
    "def train_q_learning_model(data_path, num_episodes=1000):\n",
    "    \"\"\"\n",
    "    Train a Q-learning model on stock data\n",
    "    \"\"\"\n",
    "    # Load and preprocess data\n",
    "    data = load_and_preprocess_data(data_path)\n",
    "    \n",
    "    # Initialize environment and agent\n",
    "    env = TradingEnvironment(data)\n",
    "    agent = QLearningAgent(state_size=3, action_size=num_actions)\n",
    "    \n",
    "    # Training statistics\n",
    "    episode_rewards = []\n",
    "    portfolio_values = []\n",
    "    \n",
    "    # Training loop\n",
    "    for episode in range(num_episodes):\n",
    "        # Reset environment\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        # Episode loop\n",
    "        while not done:\n",
    "            # Select action\n",
    "            action = agent.get_action(state)\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            # Update Q-table\n",
    "            agent.update_q_table(state, action, reward, next_state)\n",
    "            \n",
    "            # Update state\n",
    "            state = next_state\n",
    "            \n",
    "            # Accumulate reward\n",
    "            total_reward += reward\n",
    "        \n",
    "        # Decay exploration rate\n",
    "        agent.decay_exploration()\n",
    "        \n",
    "        # Track statistics\n",
    "        episode_rewards.append(total_reward)\n",
    "        \n",
    "        # Check if net_worth_history has items before accessing\n",
    "        if env.net_worth_history:\n",
    "            portfolio_values.append(env.net_worth_history[-1])\n",
    "        else:\n",
    "            portfolio_values.append(env.initial_balance)\n",
    "        \n",
    "        # Print progress every 100 episodes\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-100:])\n",
    "            avg_portfolio = np.mean(portfolio_values[-100:])\n",
    "            print(f\"Episode: {episode+1}, Avg Reward: {avg_reward:.2f}, Avg Portfolio Value: {avg_portfolio:.2f}, Exploration Rate: {agent.exploration_rate:.2f}\")\n",
    "    \n",
    "    return agent, episode_rewards, portfolio_values\n",
    "\n",
    "# 7. Evaluate the trained model\n",
    "def evaluate_model(agent, data_path, visualize=True):\n",
    "    \"\"\"\n",
    "    Evaluate the trained Q-learning model on test data\n",
    "    \"\"\"\n",
    "    # Load and preprocess test data\n",
    "    test_data = load_and_preprocess_data(data_path)\n",
    "    \n",
    "    # Initialize environment\n",
    "    env = TradingEnvironment(test_data)\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    # Action history\n",
    "    actions_taken = []\n",
    "    \n",
    "    # Evaluation loop\n",
    "    while not done:\n",
    "        # Select best action from Q-table (no exploration)\n",
    "        action = np.argmax(agent.q_table[int(state[0]+1), int(state[1]), int(state[2]), :])\n",
    "        \n",
    "        # Take action\n",
    "        next_state, reward, done = env.step(action)\n",
    "        \n",
    "        # Track action\n",
    "        actions_taken.append(action)\n",
    "        \n",
    "        # Update state\n",
    "        state = next_state\n",
    "    \n",
    "    # Calculate final portfolio value\n",
    "    final_portfolio_value = env.net_worth_history[-1]\n",
    "    profit_percent = (final_portfolio_value - env.initial_balance) / env.initial_balance * 100\n",
    "    \n",
    "    print(f\"Final Portfolio Value: ${final_portfolio_value:.2f}\")\n",
    "    print(f\"Profit/Loss: {profit_percent:.2f}%\")\n",
    "    \n",
    "    # Visualize results\n",
    "    if visualize:\n",
    "        # Plot portfolio value over time\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(env.net_worth_history)\n",
    "        plt.title('Portfolio Value Over Time')\n",
    "        plt.xlabel('Trading Days')\n",
    "        plt.ylabel('Portfolio Value ($)')\n",
    "        \n",
    "        # Plot stock price and actions\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(test_data['close'])\n",
    "        \n",
    "        # Mark buy actions\n",
    "        buy_days = [i for i, a in enumerate(actions_taken) if a == 1]\n",
    "        if buy_days:\n",
    "            buy_prices = [test_data['close'].iloc[i] if i < len(test_data) else None for i in buy_days]\n",
    "            buy_prices = [p for p in buy_prices if p is not None]\n",
    "            buy_days = [d for d, p in zip(buy_days, [test_data['close'].iloc[i] if i < len(test_data) else None for i in buy_days]) if p is not None]\n",
    "            if buy_days and buy_prices:\n",
    "                plt.scatter(buy_days, buy_prices, color='green', marker='^', alpha=0.7, label='Buy')\n",
    "        \n",
    "        # Mark sell actions\n",
    "        sell_days = [i for i, a in enumerate(actions_taken) if a == 2]\n",
    "        if sell_days:\n",
    "            sell_prices = [test_data['close'].iloc[i] if i < len(test_data) else None for i in sell_days]\n",
    "            sell_prices = [p for p in sell_prices if p is not None]\n",
    "            sell_days = [d for d, p in zip(sell_days, [test_data['close'].iloc[i] if i < len(test_data) else None for i in sell_days]) if p is not None]\n",
    "            if sell_days and sell_prices:\n",
    "                plt.scatter(sell_days, sell_prices, color='red', marker='v', alpha=0.7, label='Sell')\n",
    "        \n",
    "        plt.title('Stock Price and Trading Actions')\n",
    "        plt.xlabel('Trading Days')\n",
    "        plt.ylabel('Stock Price ($)')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return final_portfolio_value, profit_percent, actions_taken\n",
    "\n",
    "# 8. Main execution\n",
    "def main():\n",
    "    # File path to your stock data CSV\n",
    "    data_path = '../data/stock_data.csv'\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Training Q-learning model...\")\n",
    "    agent, rewards, portfolio_values = train_q_learning_model(data_path, num_episodes=1000)\n",
    "    \n",
    "    # Save the trained model (Q-table)\n",
    "    np.save('q_table.npy', agent.q_table)\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    print(\"\\nEvaluating model on test data...\")\n",
    "    evaluate_model(agent, data_path)\n",
    "    \n",
    "    # Plot training progress\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(rewards)\n",
    "    plt.title('Rewards per Episode')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(portfolio_values)\n",
    "    plt.title('Final Portfolio Value per Episode')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Portfolio Value ($)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
