{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100/1000: Total Reward = 749260.71, Epsilon = 0.606\n",
      "Episode 200/1000: Total Reward = 1531085.36, Epsilon = 0.367\n",
      "Episode 300/1000: Total Reward = 2250975.90, Epsilon = 0.222\n",
      "Episode 400/1000: Total Reward = 2449899.80, Epsilon = 0.135\n",
      "Episode 500/1000: Total Reward = 2978400.68, Epsilon = 0.082\n",
      "Episode 600/1000: Total Reward = 3582881.63, Epsilon = 0.049\n",
      "Episode 700/1000: Total Reward = 3651409.49, Epsilon = 0.030\n",
      "Episode 800/1000: Total Reward = 3816929.28, Epsilon = 0.018\n",
      "Episode 900/1000: Total Reward = 4070725.70, Epsilon = 0.011\n",
      "Episode 1000/1000: Total Reward = 3715319.01, Epsilon = 0.010\n",
      "Learned Q-table:\n",
      "State ('down', 0): [1466.88981834    0.         1412.6953365 ]\n",
      "State ('down', 1): [   0.          728.65711922 1492.64620229]\n",
      "State ('up', 0): [1263.8813019     0.         1411.35175266]\n",
      "State ('up', 1): [   0.         1305.6225791  1404.55902987]\n",
      "State ('stable', 1): [   0.          691.36638923 1409.46904481]\n",
      "State ('stable', 0): [1108.00698101    0.          868.30137134]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Define a simple trading environment for stock data.\n",
    "class TradingEnv:\n",
    "    def __init__(self, csv_path, threshold=0.001):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          csv_path: Path to CSV file with historical stock data (must contain a 'Close' column).\n",
    "          threshold: Minimum absolute daily return to classify trend as 'up' or 'down'\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(csv_path, parse_dates=['Date'])\n",
    "        self.df.sort_values('Date', inplace=True)\n",
    "        self.prices = self.df['Close'].values\n",
    "        self.n_days = len(self.prices)\n",
    "        self.threshold = threshold\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Start at day 1 (so we can compute return from day 0)\n",
    "        self.current_day = 1\n",
    "        self.holding = 0       # 0: not holding, 1: holding\n",
    "        self.buy_price = None  # Record the buy price when position is taken\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_trend(self):\n",
    "        # Compute return from previous day\n",
    "        ret = (self.prices[self.current_day] - self.prices[self.current_day - 1]) / self.prices[self.current_day - 1]\n",
    "        if ret > self.threshold:\n",
    "            return 'up'\n",
    "        elif ret < -self.threshold:\n",
    "            return 'down'\n",
    "        else:\n",
    "            return 'stable'\n",
    "\n",
    "    def _get_state(self):\n",
    "        # State is (trend, holding). Trend: 'up', 'down', 'stable'; holding: 0 or 1.\n",
    "        trend = self._get_trend()\n",
    "        return (trend, self.holding)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Actions:\n",
    "          0: Buy (if not holding)\n",
    "          1: Sell (if holding)\n",
    "          2: Hold\n",
    "        Returns:\n",
    "          next_state, reward, done\n",
    "        \"\"\"\n",
    "        reward = 0\n",
    "        # Action restrictions: if not holding, only buy or hold; if holding, only sell or hold.\n",
    "        if self.holding == 0:\n",
    "            if action == 0:  # Buy\n",
    "                self.holding = 1\n",
    "                self.buy_price = self.prices[self.current_day]\n",
    "            # else: hold: do nothing\n",
    "        else:  # holding == 1\n",
    "            if action == 1:  # Sell\n",
    "                sell_price = self.prices[self.current_day]\n",
    "                reward = sell_price - self.buy_price  # profit (can be negative)\n",
    "                self.holding = 0\n",
    "                self.buy_price = None\n",
    "            # else: hold, do nothing\n",
    "\n",
    "        # Advance one day\n",
    "        self.current_day += 1\n",
    "        done = self.current_day >= self.n_days\n",
    "        next_state = self._get_state() if not done else None\n",
    "        return next_state, reward, done\n",
    "\n",
    "# Define a Q-learning agent for the trading environment.\n",
    "class QLearningAgent:\n",
    "    def __init__(self, actions, alpha=0.1, gamma=0.95, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          actions: list of possible actions\n",
    "          alpha: learning rate\n",
    "          gamma: discount factor\n",
    "          epsilon: initial exploration rate\n",
    "          epsilon_min: minimum exploration rate\n",
    "          epsilon_decay: multiplicative decay factor per episode\n",
    "        \"\"\"\n",
    "        self.actions = actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        # Q-table: keys will be state tuples, values are arrays for each action.\n",
    "        self.Q = {}\n",
    "\n",
    "    def get_Q(self, state):\n",
    "        # Return Q-values for state; if state not in Q, initialize with zeros.\n",
    "        if state not in self.Q:\n",
    "            self.Q[state] = np.zeros(len(self.actions))\n",
    "        return self.Q[state]\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.actions)\n",
    "        Qs = self.get_Q(state)\n",
    "        return self.actions[np.argmax(Qs)]\n",
    "\n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        a_index = self.actions.index(action)\n",
    "        Qs = self.get_Q(state)\n",
    "        q_predict = Qs[a_index]\n",
    "        if not done:\n",
    "            q_target = reward + self.gamma * np.max(self.get_Q(next_state))\n",
    "        else:\n",
    "            q_target = reward\n",
    "        self.Q[state][a_index] += self.alpha * (q_target - q_predict)\n",
    "        if done:\n",
    "            self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "\n",
    "def train_q_learning(env, agent, episodes=1000):\n",
    "    rewards_all = []\n",
    "    for ep in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            # Choose valid action based on state holding:\n",
    "            # If not holding: available actions are [Buy, Hold] i.e. [0, 2]\n",
    "            # If holding: available actions are [Sell, Hold] i.e. [1, 2]\n",
    "            if state[1] == 0:\n",
    "                valid_actions = [0, 2]\n",
    "            else:\n",
    "                valid_actions = [1, 2]\n",
    "            action = random.choice(valid_actions) if np.random.rand() < agent.epsilon else \\\n",
    "                     valid_actions[np.argmax([agent.get_Q(state)[agent.actions.index(a)] for a in valid_actions])]\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.learn(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        rewards_all.append(total_reward)\n",
    "        if (ep+1) % 100 == 0:\n",
    "            print(f\"Episode {ep+1}/{episodes}: Total Reward = {total_reward:.2f}, Epsilon = {agent.epsilon:.3f}\")\n",
    "    return agent, rewards_all\n",
    "import pickle\n",
    "def main():\n",
    "    # Initialize the trading environment with your stock CSV.\n",
    "    env = TradingEnv('/Users/devshah/Documents/WorkSpace/University/year 3/CSC392/Trading_Simulator/data/updated_data.csv', threshold=0.001)\n",
    "    actions = [0, 1, 2]  # 0: Buy, 1: Sell, 2: Hold\n",
    "    agent = QLearningAgent(actions, alpha=0.1, gamma=0.95, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995)\n",
    "    \n",
    "    # Train the Q-learning agent.\n",
    "    trained_agent, rewards = train_q_learning(env, agent, episodes=1000)\n",
    "    \n",
    "    print(\"Learned Q-table:\")\n",
    "    for state, q_values in trained_agent.Q.items():\n",
    "        print(f\"State {state}: {q_values}\")\n",
    "    \n",
    "    # Export the trained Q-learning model (agent)\n",
    "    with open('q_learning_agent.pkl', 'wb') as f:\n",
    "        pickle.dump(trained_agent, f)\n",
    "    print(\"Q-learning agent saved as 'q_learning_agent.pkl'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trained_agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtrained_agent\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trained_agent' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
