{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Epoch 1/100 - Loss: 0.6932 - Training Accuracy: 0.54\n",
      "Epoch 10/100 - Loss: 0.6634 - Training Accuracy: 0.61\n",
      "Epoch 20/100 - Loss: 0.6307 - Training Accuracy: 0.66\n",
      "Epoch 30/100 - Loss: 0.5934 - Training Accuracy: 0.73\n",
      "Epoch 40/100 - Loss: 0.5636 - Training Accuracy: 0.76\n",
      "Epoch 50/100 - Loss: 0.5422 - Training Accuracy: 0.78\n",
      "Epoch 60/100 - Loss: 0.5108 - Training Accuracy: 0.80\n",
      "Epoch 70/100 - Loss: 0.5000 - Training Accuracy: 0.82\n",
      "Epoch 80/100 - Loss: 0.4806 - Training Accuracy: 0.83\n",
      "Epoch 90/100 - Loss: 0.4814 - Training Accuracy: 0.84\n",
      "Epoch 100/100 - Loss: 0.4614 - Training Accuracy: 0.84\n",
      "\n",
      "Final Training Loss: 0.3541, Training Accuracy: 0.84\n",
      "Final Test Loss: 0.8396, Test Accuracy: 0.52\n",
      "Strategy Return: 193.91%\n",
      "Market Return: -28.49%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "data_file = '../data/data.csv'\n",
    "df = pd.read_csv(data_file, parse_dates=True, index_col=0)\n",
    "\n",
    "feature_columns = [col for col in df.columns if col != 'target']\n",
    "X = df[feature_columns].values\n",
    "y = df['target'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "split_index = int(0.8 * len(X_scaled))\n",
    "X_train = X_scaled[:split_index]\n",
    "X_test = X_scaled[split_index:]\n",
    "y_train = y[:split_index]\n",
    "y_test = y[split_index:]\n",
    "\n",
    "# noise = np.random.normal(0, 0.01, X_train.shape)\n",
    "# X_train = X_train + noise\n",
    "\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "class TradingNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(TradingNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "model = TradingNet(input_dim).to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "\n",
    "def evaluate(model, X_tensor, y_tensor):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_tensor.to(device))\n",
    "        loss = criterion(outputs, y_tensor.to(device))\n",
    "        preds = (outputs > 0.5).float()\n",
    "        accuracy = (preds.eq(y_tensor.to(device)).sum().item()) / len(y_tensor)\n",
    "    return loss.item(), accuracy\n",
    "\n",
    "epochs = 100\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * batch_X.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    \n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        train_loss_epoch, train_acc_epoch = evaluate(model, X_train_tensor, y_train_tensor)\n",
    "        print(f\"Epoch {epoch}/{epochs} - Loss: {epoch_loss:.4f} - Training Accuracy: {train_acc_epoch:.2f}\")\n",
    "\n",
    "train_loss, train_acc = evaluate(model, X_train_tensor, y_train_tensor)\n",
    "test_loss, test_acc = evaluate(model, X_test_tensor, y_test_tensor)\n",
    "print(f\"\\nFinal Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}\")\n",
    "print(f\"Final Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_prob = model(X_test_tensor.to(device))\n",
    "    y_pred = (y_pred_prob > 0.5).float().cpu().numpy().flatten()\n",
    "\n",
    "if 'close' in df.columns:\n",
    "    analysis_df = df.iloc[split_index:].copy()\n",
    "    analysis_df['predicted_direction'] = y_pred\n",
    "    analysis_df['next_day_return'] = analysis_df['close'].pct_change().shift(-1)\n",
    "    analysis_df['strategy_return'] = analysis_df['next_day_return'] * analysis_df['predicted_direction']\n",
    "    analysis_df['cumulative_strategy_return'] = (1 + analysis_df['strategy_return']).cumprod()\n",
    "    analysis_df['cumulative_market_return'] = (1 + analysis_df['next_day_return']).cumprod()\n",
    "    strategy_return = analysis_df['cumulative_strategy_return'].iloc[-2] - 1\n",
    "    market_return = analysis_df['cumulative_market_return'].iloc[-2] - 1\n",
    "    print(f\"Strategy Return: {strategy_return:.2%}\")\n",
    "    print(f\"Market Return: {market_return:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Epoch 1/100 - Train Loss: 0.7045 - Train Acc: 0.54 - Val Loss: 0.6922 - Val Acc: 0.53\n",
      "Epoch 10/100 - Train Loss: 0.6891 - Train Acc: 0.57 - Val Loss: 0.6907 - Val Acc: 0.54\n",
      "Early stopping triggered at epoch 20\n",
      "\n",
      "Final Training Loss: 0.6703, Training Accuracy: 0.59\n",
      "Final Test Loss: 0.6918, Test Accuracy: 0.53\n",
      "Strategy Return: 437.57%\n",
      "Market Return: -28.49%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load data\n",
    "data_file = '/Users/devshah/Documents/WorkSpace/University/year 3/CSC392/Trading_Simulator/data/data.csv'\n",
    "df = pd.read_csv(data_file, parse_dates=True, index_col=0)\n",
    "\n",
    "feature_columns = [col for col in df.columns if col != 'target']\n",
    "X = df[feature_columns].values\n",
    "y = df['target'].values\n",
    "\n",
    "# Standard scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "split_index = int(0.8 * len(X_scaled))\n",
    "X_train = X_scaled[:split_index]\n",
    "X_test = X_scaled[split_index:]\n",
    "y_train = y[:split_index]\n",
    "y_test = y[split_index:]\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# ------------------------------\n",
    "# Updated Model with BatchNorm and Increased Dropout\n",
    "# ------------------------------\n",
    "class TradingNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(TradingNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),  # Increased dropout\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),  # Increased dropout\n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "model = TradingNet(input_dim).to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# ------------------------------\n",
    "# Evaluation Function\n",
    "# ------------------------------\n",
    "def evaluate(model, X_tensor, y_tensor):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_tensor.to(device))\n",
    "        loss = criterion(outputs, y_tensor.to(device))\n",
    "        preds = (outputs > 0.5).float()\n",
    "        accuracy = (preds.eq(y_tensor.to(device)).sum().item()) / len(y_tensor)\n",
    "    return loss.item(), accuracy\n",
    "\n",
    "# ------------------------------\n",
    "# Training Loop with Early Stopping\n",
    "# ------------------------------\n",
    "epochs = 100\n",
    "best_val_loss = float('inf')\n",
    "patience = 10\n",
    "trigger_times = 0\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * batch_X.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    val_loss, val_acc = evaluate(model, X_test_tensor, y_test_tensor)\n",
    "    \n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        trigger_times = 0\n",
    "        # Optionally, save the model checkpoint here\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        if trigger_times >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "            break\n",
    "    \n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        train_loss_epoch, train_acc_epoch = evaluate(model, X_train_tensor, y_train_tensor)\n",
    "        print(f\"Epoch {epoch}/{epochs} - Train Loss: {epoch_loss:.4f} - Train Acc: {train_acc_epoch:.2f} - Val Loss: {val_loss:.4f} - Val Acc: {val_acc:.2f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Final Evaluation and Analysis\n",
    "# ------------------------------\n",
    "train_loss, train_acc = evaluate(model, X_train_tensor, y_train_tensor)\n",
    "test_loss, test_acc = evaluate(model, X_test_tensor, y_test_tensor)\n",
    "print(f\"\\nFinal Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}\")\n",
    "print(f\"Final Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_prob = model(X_test_tensor.to(device))\n",
    "    y_pred = (y_pred_prob > 0.5).float().cpu().numpy().flatten()\n",
    "\n",
    "if 'close' in df.columns:\n",
    "    analysis_df = df.iloc[split_index:].copy()\n",
    "    analysis_df['predicted_direction'] = y_pred\n",
    "    analysis_df['next_day_return'] = analysis_df['close'].pct_change().shift(-1)\n",
    "    analysis_df['strategy_return'] = analysis_df['next_day_return'] * analysis_df['predicted_direction']\n",
    "    analysis_df['cumulative_strategy_return'] = (1 + analysis_df['strategy_return']).cumprod()\n",
    "    analysis_df['cumulative_market_return'] = (1 + analysis_df['next_day_return']).cumprod()\n",
    "    strategy_return = analysis_df['cumulative_strategy_return'].iloc[-2] - 1\n",
    "    market_return = analysis_df['cumulative_market_return'].iloc[-2] - 1\n",
    "    print(f\"Strategy Return: {strategy_return:.2%}\")\n",
    "    print(f\"Market Return: {market_return:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Epoch 00007: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 10/200\n",
      "Train Loss: 0.6742\n",
      "Val Loss: 0.7588, Acc: 0.533, F1: 0.621\n",
      "Epoch 00013: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Early stopping triggered at epoch 16\n",
      "\n",
      "Final Test Results:\n",
      "Accuracy: 0.523\n",
      "Precision: 0.529\n",
      "Recall: 0.884\n",
      "F1 Score: 0.662\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected a 1D array, got an array with shape (1257, 1257)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'strategy_return'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/frame.py:4485\u001b[0m, in \u001b[0;36mDataFrame._set_item_mgr\u001b[0;34m(self, key, value, refs)\u001b[0m\n\u001b[1;32m   4484\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 4485\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info_axis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4486\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m   4487\u001b[0m     \u001b[38;5;66;03m# This item wasn't present, just insert at end\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'strategy_return'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 237\u001b[0m\n\u001b[1;32m    234\u001b[0m analysis_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnext_day_return\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m analysis_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclose\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mpct_change()\u001b[38;5;241m.\u001b[39mshift(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# Risk management: Only take trades with high confidence\u001b[39;00m\n\u001b[0;32m--> 237\u001b[0m \u001b[43manalysis_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstrategy_return\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(\n\u001b[1;32m    238\u001b[0m     y_pred_prob\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.7\u001b[39m,  \u001b[38;5;66;03m# High confidence threshold\u001b[39;00m\n\u001b[1;32m    239\u001b[0m     analysis_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnext_day_return\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m analysis_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_direction\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    241\u001b[0m )\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# Add position sizing based on prediction confidence\u001b[39;00m\n\u001b[1;32m    244\u001b[0m confidence_scores \u001b[38;5;241m=\u001b[39m y_pred_prob\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/frame.py:4311\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   4309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4310\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[0;32m-> 4311\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/frame.py:4538\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4535\u001b[0m             value \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtile(value, (\u001b[38;5;28mlen\u001b[39m(existing_piece\u001b[38;5;241m.\u001b[39mcolumns), \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m   4536\u001b[0m             refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 4538\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/frame.py:4488\u001b[0m, in \u001b[0;36mDataFrame._set_item_mgr\u001b[0;34m(self, key, value, refs)\u001b[0m\n\u001b[1;32m   4485\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   4486\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m   4487\u001b[0m     \u001b[38;5;66;03m# This item wasn't present, just insert at end\u001b[39;00m\n\u001b[0;32m-> 4488\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info_axis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4489\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4490\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iset_item_mgr(loc, value, refs\u001b[38;5;241m=\u001b[39mrefs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/internals/managers.py:1370\u001b[0m, in \u001b[0;36mBlockManager.insert\u001b[0;34m(self, loc, item, value, refs)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m   1369\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 1370\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1371\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a 1D array, got an array with shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1372\u001b[0m         )\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     value \u001b[38;5;241m=\u001b[39m ensure_block_shape(value, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim)\n",
      "\u001b[0;31mValueError\u001b[0m: Expected a 1D array, got an array with shape (1257, 1257)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Data loading and preprocessing\n",
    "data_file = '/Users/devshah/Documents/WorkSpace/University/year 3/CSC392/Trading_Simulator/data/data.csv'\n",
    "df = pd.read_csv(data_file, parse_dates=True, index_col=0)\n",
    "\n",
    "# Feature engineering\n",
    "def add_technical_indicators(df):\n",
    "    # Moving averages\n",
    "    df['SMA_5'] = df['close'].rolling(window=5).mean()\n",
    "    df['SMA_20'] = df['close'].rolling(window=20).mean()\n",
    "    df['SMA_50'] = df['close'].rolling(window=50).mean()\n",
    "    \n",
    "    # Relative Strength Index (RSI)\n",
    "    delta = df['close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    df['RSI'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # MACD\n",
    "    exp1 = df['close'].ewm(span=12, adjust=False).mean()\n",
    "    exp2 = df['close'].ewm(span=26, adjust=False).mean()\n",
    "    df['MACD'] = exp1 - exp2\n",
    "    df['Signal_Line'] = df['MACD'].ewm(span=9, adjust=False).mean()\n",
    "    \n",
    "    # Volatility\n",
    "    df['Volatility'] = df['close'].rolling(window=20).std()\n",
    "    \n",
    "    # Remove NaN values\n",
    "    return df.dropna()\n",
    "\n",
    "df = add_technical_indicators(df)\n",
    "\n",
    "feature_columns = [col for col in df.columns if col != 'target']\n",
    "X = df[feature_columns].values\n",
    "y = df['target'].values\n",
    "\n",
    "# Use RobustScaler instead of StandardScaler to handle outliers better\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Time series split\n",
    "tscv = TimeSeriesSplit(n_splits=5, test_size=int(0.2 * len(X_scaled)))\n",
    "split = list(tscv.split(X_scaled))\n",
    "train_index, test_index = split[-1]  # Get the last split\n",
    "X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "\n",
    "\n",
    "# for train_index, test_index in tscv:\n",
    "#     X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "#     y_train, y_test = y[train_index], y[test_index]\n",
    "#     break  # We'll use the last split\n",
    "\n",
    "# Data augmentation with subtle variations\n",
    "def augment_data(X, y, noise_factor=0.02):\n",
    "    X_aug = np.concatenate([\n",
    "        X,\n",
    "        X + np.random.normal(0, noise_factor, X.shape),\n",
    "        X * (1 + np.random.normal(0, noise_factor, X.shape))\n",
    "    ])\n",
    "    y_aug = np.concatenate([y, y, y])\n",
    "    return X_aug, y_aug\n",
    "\n",
    "X_train, y_train = augment_data(X_train, y_train)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Improved DataLoader with weighted sampling for imbalanced classes\n",
    "class_weights = torch.tensor([1.0 / (y_train == i).mean() for i in [0, 1]])\n",
    "sample_weights = torch.tensor([class_weights[int(t)] for t in y_train])\n",
    "sampler = torch.utils.data.WeightedRandomSampler(sample_weights, len(sample_weights))\n",
    "\n",
    "batch_size = 64  # Increased batch size\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)\n",
    "\n",
    "class ImprovedTradingNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ImprovedTradingNet, self).__init__()\n",
    "        \n",
    "        self.batch_norm1 = nn.BatchNorm1d(input_dim)\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.4)\n",
    "        )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.4)\n",
    "        )\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "model = ImprovedTradingNet(input_dim).to(device)\n",
    "\n",
    "# Custom loss function combining BCE and focal loss\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = F.binary_cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * bce_loss\n",
    "        return (bce_loss + focal_loss).mean()\n",
    "\n",
    "criterion = CombinedLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "def evaluate(model, X_tensor, y_tensor, threshold=0.5):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_tensor.to(device))\n",
    "        loss = criterion(outputs, y_tensor.to(device))\n",
    "        preds = (outputs > threshold).float()\n",
    "        accuracy = (preds.eq(y_tensor.to(device)).sum().item()) / len(y_tensor)\n",
    "        \n",
    "        # Calculate precision, recall, and F1 score\n",
    "        tp = ((preds == 1) & (y_tensor.to(device) == 1)).sum().item()\n",
    "        fp = ((preds == 1) & (y_tensor.to(device) == 0)).sum().item()\n",
    "        fn = ((preds == 0) & (y_tensor.to(device) == 1)).sum().item()\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "    return loss.item(), accuracy, precision, recall, f1\n",
    "\n",
    "# Training with early stopping\n",
    "epochs = 200\n",
    "best_val_loss = float('inf')\n",
    "patience = 15\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * batch_X.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    val_loss, val_acc, val_prec, val_rec, val_f1 = evaluate(model, X_test_tensor, y_test_tensor)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "        break\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}/{epochs}\")\n",
    "        print(f\"Train Loss: {epoch_loss:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.3f}, F1: {val_f1:.3f}\")\n",
    "\n",
    "# Load best model and evaluate\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "test_loss, test_acc, test_prec, test_rec, test_f1 = evaluate(model, X_test_tensor, y_test_tensor)\n",
    "print(f\"\\nFinal Test Results:\")\n",
    "print(f\"Accuracy: {test_acc:.3f}\")\n",
    "print(f\"Precision: {test_prec:.3f}\")\n",
    "print(f\"Recall: {test_rec:.3f}\")\n",
    "print(f\"F1 Score: {test_f1:.3f}\")\n",
    "\n",
    "# Trading strategy evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_prob = model(X_test_tensor.to(device))\n",
    "    y_pred = (y_pred_prob > 0.5).float().cpu().numpy().flatten()\n",
    "\n",
    "if 'close' in df.columns:\n",
    "    analysis_df = df.iloc[test_index].copy()\n",
    "    analysis_df['predicted_direction'] = y_pred\n",
    "    analysis_df['next_day_return'] = analysis_df['close'].pct_change().shift(-1)\n",
    "    \n",
    "    # Risk management: Only take trades with high confidence\n",
    "    analysis_df['strategy_return'] = np.where(\n",
    "        y_pred_prob.cpu().numpy() > 0.7,  # High confidence threshold\n",
    "        analysis_df['next_day_return'] * analysis_df['predicted_direction'],\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    # Add position sizing based on prediction confidence\n",
    "    confidence_scores = y_pred_prob.cpu().numpy()\n",
    "    analysis_df['position_size'] = np.where(\n",
    "        confidence_scores > 0.7,\n",
    "        (confidence_scores - 0.7) / 0.3,  # Scale position size based on confidence\n",
    "        0\n",
    "    )\n",
    "    analysis_df['strategy_return'] *= analysis_df['position_size']\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    analysis_df['cumulative_strategy_return'] = (1 + analysis_df['strategy_return']).cumprod()\n",
    "    analysis_df['cumulative_market_return'] = (1 + analysis_df['next_day_return']).cumprod()\n",
    "    \n",
    "    # Calculate Sharpe Ratio (assuming risk-free rate of 2%)\n",
    "    rf_rate = 0.02\n",
    "    daily_rf = (1 + rf_rate) ** (1/252) - 1\n",
    "    excess_returns = analysis_df['strategy_return'] - daily_rf\n",
    "    sharpe_ratio = np.sqrt(252) * (excess_returns.mean() / excess_returns.std())\n",
    "    \n",
    "    # Maximum drawdown\n",
    "    cumulative_returns = analysis_df['cumulative_strategy_return']\n",
    "    rolling_max = cumulative_returns.expanding().max()\n",
    "    drawdowns = (cumulative_returns - rolling_max) / rolling_max\n",
    "    max_drawdown = drawdowns.min()\n",
    "    \n",
    "    strategy_return = analysis_df['cumulative_strategy_return'].iloc[-1] - 1\n",
    "    market_return = analysis_df['cumulative_market_return'].iloc[-1] - 1\n",
    "    \n",
    "    print(\"\\nTrading Performance Metrics:\")\n",
    "    print(f\"Strategy Return: {strategy_return:.2%}\")\n",
    "    print(f\"Market Return: {market_return:.2%}\")\n",
    "    print(f\"Sharpe Ratio: {sharpe_ratio:.2f}\")\n",
    "    print(f\"Maximum Drawdown: {max_drawdown:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
